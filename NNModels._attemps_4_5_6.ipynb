{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce64cb30-e347-4b09-9b02-73de8fcea1cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import time\n",
    "import warnings\n",
    "import gc\n",
    "import copy\n",
    "import pickle\n",
    "from hurst import compute_Hc\n",
    "from scipy.signal import hilbert\n",
    "from scipy.signal import iirfilter, filtfilt\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.metrics import KLDivergence\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.utils import resample\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "import os\n",
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "tf.config.list_physical_devices('GPU')\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "import keras\n",
    "import keras.backend as K\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3487e5a-a149-4b90-9699-f4aecf56cc66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fabien's paths\n",
    "directory = 'D:/Kaggle/2024/Harmful_brain_activity_classification/train_models/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a06141-a6b8-482b-bbc3-8a8206a42cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "verif_data = pd.read_csv(directory + \"verif_headers_order.csv\")\n",
    "verif_headers_order = verif_data.columns\n",
    "\n",
    "class cScaler:\n",
    "    def __init__(self):\n",
    "        self.full_train = StandardScaler()\n",
    "\n",
    "file = open(directory + \"standard_scaler.pickle\", 'rb')\n",
    "scaler = pickle.load(file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d85095-dda1-4216-b8b9-1f7a4672e65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_parquet(directory + \"Combined_Features_wf_all.parquet\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3c2150df-3d23-4374-a525-3a72eddc75ac",
   "metadata": {},
   "source": [
    "# Balance classes\n",
    "data = pd.read_parquet(directory + \"Combined_Features_wf_all.parquet\")\n",
    "\n",
    "class_n = {}\n",
    "\n",
    "for i in data.expert_consensus.unique():\n",
    "    mask = data.expert_consensus == i\n",
    "    print(i, mask.value_counts()[True])\n",
    "    class_n[i] = mask.value_counts()[True]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "52e88b99-6c0e-4112-b797-4b53d3b8da23",
   "metadata": {},
   "source": [
    "for key in class_n.keys():\n",
    "    print(key)\n",
    "    mask = data.expert_consensus == key\n",
    "    if key == \"Seizure\": \n",
    "        balanced_data = data.loc[mask,:]\n",
    "    else:\n",
    "        if key == \"Other\":\n",
    "            balanced_data = pd.concat([balanced_data, data.loc[mask,:]], axis = 0)\n",
    "        else:\n",
    "            new = resample(data.loc[mask,:], replace = False, n_samples = int(class_n[\"Seizure\"]*0.5))\n",
    "            balanced_data = pd.concat([balanced_data, data.loc[mask,:], new], axis = 0)\n",
    "\n",
    "balanced_data = balanced_data.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "154fca40-12d2-4fc1-9fcf-d3301a357ca5",
   "metadata": {},
   "source": [
    "for i in balanced_data.expert_consensus.unique():\n",
    "    mask = balanced_data.expert_consensus == i\n",
    "    print(i, mask.value_counts()[True])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "04e9bd7e-8660-4f07-b15e-0d66a2f98a24",
   "metadata": {},
   "source": [
    "for i in balanced_data.Classes1.unique():\n",
    "    mask = balanced_data.Classes1 == i\n",
    "    print(i, mask.value_counts()[True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b205291-c057-448a-9a0b-945a66c11c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = balanced_data.copy()\n",
    "\n",
    "Y_data = data.iloc[:,9:15]\n",
    "print(data.shape)\n",
    "print(Y_data.columns)\n",
    "Y_data = Y_data.values\n",
    "\n",
    "data = data.iloc[:,15:]\n",
    "data = data.select_dtypes(include=[np.number])\n",
    "data = data.drop(\"Total_votes\", axis = 1)\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e962918f-5218-407f-b9b3-b055cdd4cb1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# strategy for missing values\n",
    "data = data.replace(np.nan, 0)\n",
    "\n",
    "Y_data = Y_data / np.sum(Y_data,axis=1,keepdims=True)\n",
    "X_cols = data.select_dtypes(include=[np.number]).keys()\n",
    "X_data = data.select_dtypes(include=[np.number])\n",
    "X_data = scaler.full_train.transform(X_data) # standardization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "682d2275-9e99-4224-9110-222871ad04f1",
   "metadata": {},
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29848fdd-8ddd-47d4-908a-6bbc33953018",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assessment metrics\n",
    "def log(x):\n",
    "    x[x == 0] = 1e-15\n",
    "    x[x == 1] = 1-1e-15\n",
    "    return np.log(x)\n",
    "\n",
    "\n",
    "def JensenShannonDiv(true_y, pred_y): # Jensen-Shannon Divergence https://towardsdatascience.com/how-to-understand-and-use-jensen-shannon-divergence-b10e11b03fd6\n",
    "    true_y = tf.convert_to_tensor(true_y, dtype=tf.float32)\n",
    "    pred_y = tf.convert_to_tensor(pred_y, dtype=tf.float32)\n",
    "    # removing 0 to avoid divisions by 0.\n",
    "    maskT = K.equal(true_y, 0)\n",
    "    maskP = K.equal(pred_y, 0)\n",
    "    true_y = K.switch(~maskT, true_y, 1e-15)\n",
    "    pred_y = K.switch(~maskP, pred_y, 1e-15)\n",
    "    # sum to 1\n",
    "    true_y = true_y / K.sum(true_y, axis = 0)\n",
    "    pred_y = pred_y / K.sum(pred_y, axis = 0)\n",
    "    JSD1 = pred_y*K.log(2* pred_y/ (pred_y+true_y))\n",
    "    JSD2 = true_y*K.log(2* true_y/ (pred_y+true_y))\n",
    "    JSD = 0.5*K.sum(JSD1, axis = 1) + 0.5*K.sum(JSD2, axis = 1)\n",
    "    # JSD should now be a 1D array. It there are NaN, calculating it's sum will fail. However, If I remove the NaN, I may falsely decrease\n",
    "    # the sum, because I calculate it with less value. My solution to overcome this is to calculate the mean of the 1D array without NaN,\n",
    "    # and multiplying the mean by the number of values in the 1D array, including NaN\n",
    "    mask = tf.math.is_nan(JSD)\n",
    "    JSD = K.mean(JSD[~mask]) * 1000\n",
    "    return JSD\n",
    "\n",
    "\n",
    "def wJensenShannonDiv(true_y, pred_y): # Jensen-Shannon Divergence https://towardsdatascience.com/how-to-understand-and-use-jensen-shannon-divergence-b10e11b03fd6\n",
    "    true_y = tf.convert_to_tensor(true_y, dtype=tf.float32)\n",
    "    pred_y = tf.convert_to_tensor(pred_y, dtype=tf.float32)\n",
    "    # removing 0 to avoid divisions by 0.\n",
    "    maskT = K.equal(true_y, 0)\n",
    "    maskP = K.equal(pred_y, 0)\n",
    "    true_y = K.switch(~maskT, true_y, 1e-15)\n",
    "    pred_y = K.switch(~maskP, pred_y, 1e-15)\n",
    "    # sum to 1\n",
    "    true_y = true_y / K.sum(true_y, axis = 0)\n",
    "    pred_y = pred_y / K.sum(pred_y, axis = 0)\n",
    "    JSD1 = pred_y*K.log(2* pred_y/ (pred_y+true_y)) * tf.convert_to_tensor([3,1,1,1,1,2], dtype=tf.float32)\n",
    "    JSD2 = true_y*K.log(2* true_y/ (pred_y+true_y)) * tf.convert_to_tensor([3,1,1,1,1,2], dtype=tf.float32)\n",
    "    JSD = 0.5*K.sum(JSD1, axis = 1) + 0.5*K.sum(JSD2, axis = 1)\n",
    "    # JSD should now be a 1D array. It there are NaN, calculating it's sum will fail. However, If I remove the NaN, I may falsely decrease\n",
    "    # the sum, because I calculate it with less value. My solution to overcome this is to calculate the mean of the 1D array without NaN,\n",
    "    # and multiplying the mean by the number of values in the 1D array, including NaN\n",
    "    mask = tf.math.is_nan(JSD)\n",
    "    JSD = K.mean(JSD[~mask])\n",
    "    return JSD\n",
    "\n",
    "\n",
    "# competition metric? \n",
    "def kl_divergence(solution, submission, epsilon = 1e-15, micro_average = False, sample_weights = None):\n",
    "    if not isinstance(solution, pd.DataFrame): solution = pd.DataFrame(solution)\n",
    "    if not isinstance(submission, pd.DataFrame): submission = pd.DataFrame(submission)   \n",
    "\n",
    "    for col in solution.columns:\n",
    "\n",
    "        if not pd.api.types.is_float_dtype(solution[col]):\n",
    "            solution[col] = solution[col].astype(float)\n",
    "        submission[col] = np.clip(submission[col], epsilon, 1 - epsilon)\n",
    "\n",
    "        y_nonzero_indices = solution[col] != 0 \n",
    "        solution[col] = solution[col].astype(float)\n",
    "        solution.loc[y_nonzero_indices, col] = solution.loc[y_nonzero_indices, col] * \\\n",
    "                                                np.log(solution.loc[y_nonzero_indices, col] / submission.loc[y_nonzero_indices, col])\n",
    "        \n",
    "        solution.loc[~y_nonzero_indices, col] = 0\n",
    "\n",
    "    if micro_average:\n",
    "        return np.average(solution.sum(axis=1))#, weights=sample_weights)\n",
    "    else:\n",
    "        return np.average(solution.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b28dbc1b-3a87-4f2f-be02-429333e856aa",
   "metadata": {},
   "source": [
    "# Keras model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3a5d35c1-2c4d-4785-8442-82a70da901bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dim_reduct(X, npc = 800):\n",
    "    pca = PCA(npc)\n",
    "    XtrainPCA = pca.fit_transform(X)\n",
    "    return pca, XtrainPCA\n",
    "\n",
    "def apply_PCA(X, pca):\n",
    "    XtestPCA = pca.transform(X)\n",
    "    return XtestPCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211958fc-d2d0-46a2-b859-9b2956a4efb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "epochs = 8\n",
    "verbose = 1\n",
    "\n",
    "CV_y_pred = pd.DataFrame(np.zeros(Y_data.shape))\n",
    "PCAmodels = {}\n",
    "all_perf = {}\n",
    "NN_models = {}"
   ]
  },
  {
   "cell_type": "raw",
   "id": "29882821-e6ce-4627-8bbe-e84f9357d247",
   "metadata": {},
   "source": [
    "# I want to avoid data leakage, so I am going to split the data in the train and test set according to one of the following colummns\n",
    "print(len(balanced_data.label_id.unique()))\n",
    "print(len(balanced_data.eeg_id.unique()))\n",
    "print(len(balanced_data.patient_id.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31a1e32-0409-4b42-8bfe-44c70bfc29e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "n_splits = 5\n",
    "\n",
    "kfold = KFold(n_splits= n_splits, shuffle = True, random_state = 42)\n",
    "\n",
    "# K-fold Cross Validation model evaluation\n",
    "\n",
    "n = -1\n",
    "# for train_idx, test_idx in kfold.split(balanced_data.label_id.unique()):\n",
    "for train_idx, test_idx in kfold.split(X_data, Y_data):\n",
    "    n += 1\n",
    "    print(\"---\", n)\n",
    "\n",
    "    # mask = balanced_data.label_id.isin(balanced_data.label_id.unique()[train_idx])\n",
    "    X_train, y_train = X_data[train_idx,:] , Y_data[train_idx,:]\n",
    "    X_test, y_test = X_data[test_idx,:] , Y_data[test_idx,:]\n",
    "\n",
    "\n",
    "    PCAmodels[n], X_train = dim_reduct(X_train, 800)\n",
    "    X_test = apply_PCA(X_test, PCAmodels[n])\n",
    "    # X_train = np.expand_dims(X_train, axis=2)\n",
    "    # X_test = np.expand_dims(X_test, axis=2)\n",
    "    \n",
    "    print(\"X_train shape:\", X_train.shape)\n",
    "    print(\"y_train shape:\", y_train.shape)\n",
    "    print(\"X_test shape\", X_test.shape)\n",
    "    \n",
    "    # mA0 = keras.layers.Input(shape=(X_train.shape[1], X_train.shape[2]), name = \"inputA\")\n",
    "    mA0 = keras.layers.Input(shape=X_train.shape[1], name = \"inputA\")\n",
    "    mA1 = keras.layers.Dense(1200, activation=\"gelu\")(mA0)\n",
    "    mA2 = keras.layers.Dropout(0.05)(mA1)\n",
    "    mA3 = keras.layers.Dense(600, activation=\"gelu\")(mA2)\n",
    "    mA4 = keras.layers.Dropout(0.05)(mA3)\n",
    "    mA5 = keras.layers.Dense(200, activation=\"gelu\")(mA4)\n",
    "    mA6 = keras.layers.Dropout(0.05)(mA5)\n",
    "    mA7 = keras.layers.Dense(6, activation=\"softmax\")(mA6)\n",
    "\n",
    "    model = keras.models.Model(inputs=[mA0],outputs=[mA7], name = f'model{n}')\n",
    "\n",
    "    model.compile(\n",
    "        loss= keras.losses.KLDivergence(),\n",
    "        #loss=JensenShannonDiv,\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=1e-4),\n",
    "        metrics=[keras.metrics.KLDivergence(name = \"KLD\")],\n",
    "        run_eagerly=True\n",
    "    )\n",
    "    if n == 0: print(model.summary())\n",
    "    \n",
    "    callbacks = [keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=5)]\n",
    "    \n",
    "    model.fit(\n",
    "        [X_train],\n",
    "        y_train,\n",
    "        batch_size=batch_size,\n",
    "        epochs=epochs,\n",
    "        validation_split=0.15,\n",
    "        callbacks=callbacks,\n",
    "        verbose=verbose\n",
    "    )\n",
    "\n",
    "    model.save(directory + f\"model{n}.keras\")\n",
    "\n",
    "    NN_y_pred = model.predict(X_test)\n",
    "    CV_y_pred.loc[test_idx] = NN_y_pred\n",
    "\n",
    "    kld = KLDivergence()\n",
    "    all_perf[n] = (kld(y_test, CV_y_pred.loc[test_idx,:]),\n",
    "                   JensenShannonDiv(y_test, CV_y_pred.loc[test_idx]),\n",
    "                   wJensenShannonDiv(y_test, CV_y_pred.loc[test_idx]))\n",
    "    print(all_perf[n])\n",
    "    \n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1bd02c6-7237-45c0-89ec-c5f1efac9b71",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in all_perf.values():\n",
    "    print(i[0].numpy(), i[1].numpy(), i[2].numpy()*10000)\n",
    "\n",
    "print(\"----\")\n",
    "print(kld(Y_data, CV_y_pred),\n",
    "    JensenShannonDiv(Y_data, CV_y_pred),\n",
    "    wJensenShannonDiv(Y_data, CV_y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2ef22b-78bb-4288-a37a-43f7cdfad6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Attempt 2\n",
    "Best KLD, JSD\n",
    "mA0 = keras.layers.Input(shape=X_train.shape[1], name = \"inputA\")\n",
    "mA1 = keras.layers.Dense(1200, activation=\"LeakyReLU\")(mA0)\n",
    "mA2 = keras.layers.Dropout(0.05)(mA1)\n",
    "mA3 = keras.layers.Dense(600, activation=\"LeakyReLU\")(mA2)\n",
    "mA4 = keras.layers.Dropout(0.05)(mA3)\n",
    "mA5 = keras.layers.Dense(200, activation=\"LeakyReLU\")(mA4)\n",
    "mA6 = keras.layers.Dropout(0.05)(mA5)\n",
    "mA7 = keras.layers.Dense(6, activation=\"softmax\")(mA6)\n",
    "\n",
    "loss: KLD\n",
    "epoch: 7\n",
    "with PCA 800\n",
    "batch_size: 128\n",
    "    \n",
    "0.27772743 0.38466638\n",
    "0.29020777 0.3947794\n",
    "0.29215237 0.40049928\n",
    "0.30260068 0.41565132\n",
    "0.28473517 0.38930452\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83b0028-8e90-46e9-8c52-3ced341ed007",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Attempt 3\n",
    "mA0 = keras.layers.Input(shape=X_train.shape[1], name = \"inputA\")\n",
    "mA1 = keras.layers.Dense(1200, activation=\"LeakyReLU\")(mA0)\n",
    "mA2 = keras.layers.Dropout(0.05)(mA1)\n",
    "mA3 = keras.layers.Dense(600, activation=\"LeakyReLU\")(mA2)\n",
    "mA4 = keras.layers.Dense(100, activation=\"LeakyReLU\")(mA3)\n",
    "mA6 = keras.layers.Dropout(0.05)(mA4)\n",
    "mA7 = keras.layers.Dense(6, activation=\"softmax\")(mA6)\n",
    "\n",
    "loss: KLD\n",
    "epoch: 10\n",
    "with PCA 800\n",
    "batch_size: 128\n",
    "\n",
    "0.2687153 0.012454825 0.19520142814144492\n",
    "0.31809422 0.013991482 0.2201401730417274\n",
    "0.31590238 0.014007937 0.2191763633163646\n",
    "0.30434176 0.013607951 0.21086325432406738\n",
    "0.30868018 0.013530641 0.21034438759670593\n",
    "----\n",
    "tf.Tensor(0.30409643, shape=(), dtype=float32) tf.Tensor(0.0027094362, shape=(), dtype=float32) tf.Tensor(4.229578e-06, shape=(), dtype=float32)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e9e528-29d0-472c-a5c4-5392b49985d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "not submitted\n",
    "\n",
    "mA0 = keras.layers.Input(shape=X_train.shape[1], name = \"inputA\")\n",
    "mA3 = keras.layers.Dense(400, activation=\"gelu\")(mA0)\n",
    "mA3b = keras.layers.Dropout(0.05)(mA3)\n",
    "mA4 = keras.layers.Dense(100, activation=\"gelu\")(mA3b)\n",
    "mA5 = keras.layers.Dropout(0.05)(mA4)\n",
    "mA6 = keras.layers.Dense(25, activation=\"gelu\")(mA5)\n",
    "mA7 = keras.layers.Dense(6, activation=\"softmax\")(mA6)\n",
    "\n",
    "loss: KLD\n",
    "epoch: 30, patience 5\n",
    "with PCA 800\n",
    "batch_size: 128\n",
    "\n",
    "0.28346992 0.012983576 0.19991402950836346\n",
    "0.29861128 0.012989608 0.20122837668168359\n",
    "0.30290562 0.012781282 0.19633147530839778\n",
    "0.29059058 0.012629622 0.19448616512818262\n",
    "0.29403827 0.012706669 0.1957047606993001\n",
    "----\n",
    "tf.Tensor(0.2939548, shape=(), dtype=float32) tf.Tensor(0.0025643937, shape=(), dtype=float32) tf.Tensor(3.951708e-06, shape=(), dtype=float32)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497a3d26-5f8f-4f38-b5f5-6953938f75b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Attempt 4\n",
    "interesting case because KLD shows better overall perf, but JSD and wJSD show worse perf.\n",
    "\n",
    "\n",
    "mA0 = keras.layers.Input(shape=X_train.shape[1], name = \"inputA\")\n",
    "mA1 = keras.layers.Dense(1200, activation=\"gelu\")(mA0)\n",
    "mA2 = keras.layers.Dropout(0.05)(mA1)\n",
    "mA3 = keras.layers.Dense(600, activation=\"gelu\")(mA2)\n",
    "mA4 = keras.layers.Dropout(0.05)(mA3)\n",
    "mA5 = keras.layers.Dense(200, activation=\"gelu\")(mA4)\n",
    "mA6 = keras.layers.Dropout(0.05)(mA5)\n",
    "mA7 = keras.layers.Dense(6, activation=\"softmax\")(mA6)\n",
    "\n",
    "loss: KLD\n",
    "epoch: 3\n",
    "with PCA 800\n",
    "batch_size: 64\n",
    "\n",
    "0.27293953 0.017414736 0.2629776099638548\n",
    "0.27891082 0.0174157 0.2626997411425691\n",
    "0.27661416 0.017378118 0.260330707533285\n",
    "0.2903107 0.018472176 0.2772005245788023\n",
    "0.2842966 0.018364944 0.27507252525538206\n",
    "----\n",
    "tf.Tensor(0.28122807, shape=(), dtype=float32) tf.Tensor(0.0035667575, shape=(), dtype=float32) tf.Tensor(5.3602034e-06, shape=(), dtype=float32)\n",
    "''''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b168e54e-3e55-459e-b643-a48d9594ba26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save best model\n",
    "class cPCAmodels:\n",
    "    def __init__(self):\n",
    "        self.model = 0\n",
    "\n",
    "PCAm = cPCAmodels()\n",
    "PCAm.model = PCAmodels[0]\n",
    "file = open(directory + \"PCAmodel_attempt3.pickle\", 'wb')\n",
    "pickle.dump(PCAm, file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d358d3-3235-4ee2-b761-4adf84cac460",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the best model is model 1\n",
    "model = keras.models.load_model(directory + \"model1.keras\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79799504-f609-4dbd-bddb-12c6bd2c84dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# five more epoch with the entire dataset\n",
    "\n",
    "n_attempt = 4\n",
    "batch_size = 124\n",
    "epochs = 5\n",
    "verbose = 1\n",
    "\n",
    "X_train = apply_PCA(X_data, PCAmodels[0])\n",
    "\n",
    "model.fit(\n",
    "    [X_train],\n",
    "    Y_data,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    verbose=verbose\n",
    ")\n",
    "\n",
    "model.save(directory + f\"finalmodel{n_attempt}.keras\", save_format='keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a1af63-454c-4db5-82a5-737c9b8795e5",
   "metadata": {},
   "source": [
    "# Gradient Boosted Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4fcd00c-66fa-49af-8874-cb26890b8ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import catboost as cb\n",
    "import optuna\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5aa2f56e-699f-43da-8289-e8c30f5ae001",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = 'D:/Kaggle/2024/Harmful_brain_activity_classification/train_models/'\n",
    "data = pd.read_parquet(directory + \"Combined_Features_wf_all.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "60a73b27-45fc-42c5-bf8d-045a20746fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "verif_data = pd.read_csv(directory + \"verif_headers_order.csv\")\n",
    "verif_headers_order = verif_data.columns\n",
    "\n",
    "class cScaler:\n",
    "    def __init__(self):\n",
    "        self.full_train = StandardScaler()\n",
    "\n",
    "file = open(directory + \"standard_scaler.pickle\", 'rb')\n",
    "scaler = pickle.load(file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec7c70b2-7dfa-446e-8684-365cd1501df3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Seizure', 'GPD', 'LRDA', 'Other', 'GRDA', 'LPD'], dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.expert_consensus.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1040fe3-58ac-46d3-9a19-bba1c2a7a28f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(106800, 6)\n",
      "expert_consensus\n",
      "0                   20933\n",
      "4                   18861\n",
      "3                   18808\n",
      "1                   16702\n",
      "2                   16640\n",
      "5                   14856\n",
      "dtype: int64\n",
      "(106800, 1)\n",
      "(106800, 1410)\n",
      "(106800, 1410)\n"
     ]
    }
   ],
   "source": [
    "# data = balanced_data.copy()\n",
    "\n",
    "encoding = {\"Seizure\" : 0, \"GPD\": 1, \"LRDA\": 2, \"Other\": 3, \"GRDA\":4, \"LPD\":5}\n",
    "decoding = {0: \"Seizure\", 1: \"GPD\", 2: \"LRDA\", 3: \"Other\", 4: \"GRDA\", 5: \"LPD\"}\n",
    "\n",
    "Y_data_df = data.iloc[:,9:15]\n",
    "Y_data_df = Y_data_df / Y_data_df.sum(axis=0)\n",
    "print(Y_data_df.shape)\n",
    "\n",
    "Y_data = data[[\"expert_consensus\"]].copy()\n",
    "Y_data[\"expert_consensus\"] = Y_data[\"expert_consensus\"].apply(lambda x: encoding[x])\n",
    "print(Y_data.value_counts())\n",
    "print(Y_data.shape)\n",
    "\n",
    "data = data.iloc[:,15:]\n",
    "data = data.select_dtypes(include=[np.number])\n",
    "data = data.drop(\"Total_votes\", axis = 1)\n",
    "print(data.shape)\n",
    "\n",
    "# strategy for missing values\n",
    "data = data.replace(np.nan, 0)\n",
    "\n",
    "X_cols = data.select_dtypes(include=[np.number]).keys()\n",
    "X_data = data.select_dtypes(include=[np.number])\n",
    "X_data = scaler.full_train.transform(X_data) # standardization\n",
    "X_data = pd.DataFrame(X_data, columns = X_cols)\n",
    "print(X_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a6ae2c61-726a-4b56-bd76-a23ba45918d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log(x):\n",
    "    \n",
    "    # reshaping\n",
    "    new_x = x\n",
    "    if isinstance(x, pd.DataFrame) == False: new_x = pd.DataFrame(x)\n",
    "    \n",
    "    # replacing\n",
    "    new_x = new_x.replace(0,1e-15)\n",
    "    new_x = new_x.replace(1,1-1e-15)\n",
    "    \n",
    "    return np.log(new_x.values)\n",
    "\n",
    "\n",
    "# true_y can be a list, array or pandas dataframe of a single column containing the categories to predict. \n",
    "# pred_y has to be an array with each column corresponding to one class probability\n",
    "# idx stands for index. When one wants to assess the logloss for a single row in a dataframe,\n",
    "# the row index (number) should be assigned to idx.\n",
    "# adapt to your outcome\n",
    "def logloss(true_y, pred_y, idx = None, byclass = False): \n",
    "\n",
    "    # reshaping\n",
    "    ytrue = true_y\n",
    "    ypred = pred_y\n",
    "    if isinstance(pred_y, pd.DataFrame) == False: ypred = pd.DataFrame(pred_y)\n",
    "    if isinstance(ytrue, pd.DataFrame) == False: ytrue = pd.DataFrame(ytrue)\n",
    "    if ytrue.shape[1] == 1: ytrue = pd.DataFrame(to_categorical(ytrue))\n",
    "     \n",
    "    # logloss calculation\n",
    "    if idx == None:\n",
    "        val = -(log(ypred)*ytrue).mean()\n",
    "    else:\n",
    "        if type(idx) == int: val = -(np.reshape(log(ypred.iloc[idx,:]),(3,)) *ytrue.iloc[idx,:])\n",
    "        else: val = -(log(ypred.iloc[idx,:])*ytrue.iloc[idx,:]).mean()\n",
    "\n",
    "    # display results\n",
    "    if byclass:\n",
    "        for i in decoding.keys():\n",
    "            print(f'logloss for {decoding[i]} : {val[i]}')      \n",
    "    val = np.sum(val)\n",
    "    print(f'Overall logloss: {val}') \n",
    "    \n",
    "    return val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d065f93b-3e5c-4984-837a-b758c14aa34c",
   "metadata": {},
   "source": [
    "## Light GBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8fdcaa-73ff-4904-b7a9-8d12b8323f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lgb_objective(trial):\n",
    "  params = {'objective': \"multiclass\",\n",
    "    'metric': \"multi_logloss\",\n",
    "    'num_class': 6,\n",
    "    'max_depth': trial.suggest_int('max_depth', 15,30),\n",
    "    'num_leaves': trial.suggest_int('num_leaves', 40,120),\n",
    "    'learning_rate': trial.suggest_float('learning_rate', 0.02, 0.035),\n",
    "    'n_estimators': trial.suggest_int('n_estimators', 150, 275),\n",
    "    'colsample_bytree': trial.suggest_float('colsample_bytree', 0.02, 0.15),\n",
    "    'bagging_fraction': trial.suggest_float('bagging_fraction', 0.4,1),\n",
    "    'reg_alpha': trial.suggest_float('reg_alpha', 0.1, 0.7),\n",
    "    'reg_lambda': trial.suggest_float('reg_lambda', 4, 8),\n",
    "    'max_bin': trial.suggest_int('max_bin', 40,70),\n",
    "    'device_type': \"gpu\",\n",
    "    'verbose':-1}\n",
    "    \n",
    "  model = lgb.LGBMClassifier(**params);\n",
    "  score = cross_val_score(model, X_data , Y_data, cv=5, scoring = \"neg_log_loss\")\n",
    "  return -score.mean()\n",
    "\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(lgb_objective, n_trials=20)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "15b937ec-ae35-4afe-a331-c024d9e0f9a4",
   "metadata": {},
   "source": [
    "Best parameters\n",
    "\n",
    "Trial 9 finished with value: 1.0417435647176136 and parameters: {'max_depth': 22, 'num_leaves': 56, 'learning_rate': 0.029200818616956066, 'n_estimators': 240, 'colsample_bytree': 0.14529209839987717, 'bagging_fraction': 0.680397874556198, 'reg_alpha': 0.3880867303670045, 'reg_lambda': 7.853855198384489, 'max_bin': 57"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bdc9d756-87b6-4e8e-beb6-ec251a6a0530",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 0\n",
      "--- 1\n",
      "--- 2\n",
      "--- 3\n",
      "--- 4\n",
      "logloss for Seizure : 0.08989963088863537\n",
      "logloss for GPD : 0.042386723490068945\n",
      "logloss for LRDA : 0.05511958265461755\n",
      "logloss for Other : 0.12302536261757491\n",
      "logloss for GRDA : 0.06776814264774196\n",
      "logloss for LPD : 0.06866162342222995\n",
      "Overall logloss: 0.4468610657208687\n",
      "LGBM log loss: 0.4468610657208687\n",
      "\n",
      "LGBM KLD: -0.0004141136014368385\n"
     ]
    }
   ],
   "source": [
    "lgbparams = {'objective': 'multiclass',\n",
    "'metric': 'multi_logloss',\n",
    "'num_class': 6,\n",
    "'max_depth': 22,\n",
    "'num_leaves': 56,\n",
    "'learning_rate': 0.029200818616956066,\n",
    "'n_estimators': 240,\n",
    "'colsample_bytree': 0.14529209839987717,\n",
    "'bagging_fraction': 0.680397874556198,\n",
    "'reg_alpha': 0.3880867303670045,\n",
    "'reg_lambda': 7.853855198384489,\n",
    "'max_bin': 57,\n",
    "'device_type': 'gpu',\n",
    "\"importance_type\": \"gain\",\n",
    "'verbose': -1}\n",
    "\n",
    "CV_predictions = pd.DataFrame(np.zeros([X_data.shape[0], 6]))\n",
    "\n",
    "LGBM = lgb.LGBMClassifier(**lgbparams)\n",
    "\n",
    "n_splits = 5\n",
    "kfold = KFold(n_splits= n_splits, shuffle = True, random_state = 42)\n",
    "\n",
    "n = -1\n",
    "# for train_idx, test_idx in kfold.split(balanced_data.label_id.unique()):\n",
    "for train_idx, test_idx in kfold.split(X_data, Y_data):\n",
    "    n += 1\n",
    "    print(\"---\", n)\n",
    "    \n",
    "    X_train, y_train = X_data.loc[train_idx,:] , Y_data.loc[train_idx,:]\n",
    "    X_test, y_test = X_data.loc[test_idx,:] , Y_data.loc[test_idx,:]\n",
    "\n",
    "    LGBM.fit(X_train, y_train)\n",
    "    proba = LGBM.predict_proba(X_test)\n",
    "      \n",
    "    # build CV results\n",
    "    CV_predictions.loc[test_idx,:] = proba\n",
    "    gc.collect()\n",
    "\n",
    "LL = logloss(Y_data, CV_predictions, byclass = True)\n",
    "print(f\"LGBM log loss: {LL}\\n\")\n",
    "kld = KLDivergence()\n",
    "KLDmetric = kld(Y_data_df, CV_predictions)\n",
    "print(f\"LGBM KLD: {KLDmetric}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "60724cd2-dd58-4f8c-aa42-910591b89edd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               0         1         2         3         4         5\n",
      "0       0.467960  0.030706  0.117178  0.152520  0.039671  0.191966\n",
      "1       0.726207  0.021470  0.053387  0.091569  0.024286  0.083081\n",
      "2       0.820036  0.014303  0.039367  0.057771  0.023844  0.044679\n",
      "3       0.813929  0.014858  0.026494  0.085502  0.014746  0.044470\n",
      "4       0.815186  0.019549  0.023259  0.079154  0.016919  0.045934\n",
      "...          ...       ...       ...       ...       ...       ...\n",
      "106795  0.044618  0.021189  0.775961  0.038914  0.076975  0.042344\n",
      "106796  0.039969  0.017540  0.822999  0.031166  0.067627  0.020699\n",
      "106797  0.045350  0.020080  0.793597  0.037814  0.077944  0.025215\n",
      "106798  0.031221  0.015986  0.793649  0.032816  0.085343  0.040985\n",
      "106799  0.026354  0.010867  0.838306  0.028975  0.074160  0.021339\n",
      "\n",
      "[106800 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "print(CV_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ba3cca-da03-4209-b0bd-4baaec0b0d32",
   "metadata": {},
   "source": [
    "# Combined models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "633d81b4-869c-4540-a41a-632c54c3be07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (85440, 800)\n",
      "y_train shape: (85440, 6)\n",
      "X_test shape: (21360, 800)\n",
      "y_test shape: (21360, 6)\n",
      "X_trainLGBM shape (85440, 6)\n",
      "X_testLGBM shape (21360, 6)\n",
      "Model: \"model0\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " inputA (InputLayer)            [(None, 800)]        0           []                               \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 1200)         961200      ['inputA[0][0]']                 \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 1200)         0           ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 600)          720600      ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 600)          0           ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 200)          120200      ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 200)          0           ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 24)           4824        ['dropout_2[0][0]']              \n",
      "                                                                                                  \n",
      " inputB (InputLayer)            [(None, 6)]          0           []                               \n",
      "                                                                                                  \n",
      " concatenated_layer (Concatenat  (None, 30)          0           ['dense_3[0][0]',                \n",
      " e)                                                               'inputB[0][0]']                 \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 6)            186         ['concatenated_layer[0][0]']     \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,807,010\n",
      "Trainable params: 1,807,010\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n",
      "568/568 [==============================] - 25s 44ms/step - loss: -4.9501e-04 - KLD: -4.9501e-04 - val_loss: -5.1023e-04 - val_KLD: -5.1023e-04\n",
      "Epoch 2/100\n",
      "568/568 [==============================] - 25s 44ms/step - loss: -5.1681e-04 - KLD: -5.1681e-04 - val_loss: -5.2207e-04 - val_KLD: -5.2207e-04\n",
      "Epoch 3/100\n",
      "568/568 [==============================] - 26s 46ms/step - loss: -5.2534e-04 - KLD: -5.2534e-04 - val_loss: -5.2769e-04 - val_KLD: -5.2769e-04\n",
      "Epoch 4/100\n",
      "568/568 [==============================] - 25s 43ms/step - loss: -5.3005e-04 - KLD: -5.3005e-04 - val_loss: -5.3102e-04 - val_KLD: -5.3102e-04\n",
      "Epoch 5/100\n",
      "568/568 [==============================] - 26s 45ms/step - loss: -5.3302e-04 - KLD: -5.3302e-04 - val_loss: -5.3336e-04 - val_KLD: -5.3336e-04\n",
      "Epoch 6/100\n",
      "568/568 [==============================] - 25s 45ms/step - loss: -5.3524e-04 - KLD: -5.3524e-04 - val_loss: -5.3463e-04 - val_KLD: -5.3463e-04\n",
      "Epoch 7/100\n",
      "568/568 [==============================] - 25s 45ms/step - loss: -5.3686e-04 - KLD: -5.3686e-04 - val_loss: -5.3608e-04 - val_KLD: -5.3608e-04\n",
      "Epoch 8/100\n",
      "568/568 [==============================] - 26s 46ms/step - loss: -5.3815e-04 - KLD: -5.3815e-04 - val_loss: -5.3694e-04 - val_KLD: -5.3694e-04\n",
      "Epoch 9/100\n",
      "568/568 [==============================] - 27s 47ms/step - loss: -5.3917e-04 - KLD: -5.3917e-04 - val_loss: -5.3776e-04 - val_KLD: -5.3776e-04\n",
      "Epoch 10/100\n",
      "568/568 [==============================] - 25s 45ms/step - loss: -5.4003e-04 - KLD: -5.4003e-04 - val_loss: -5.3847e-04 - val_KLD: -5.3847e-04\n",
      "Epoch 11/100\n",
      "568/568 [==============================] - 27s 47ms/step - loss: -5.4080e-04 - KLD: -5.4080e-04 - val_loss: -5.3897e-04 - val_KLD: -5.3897e-04\n",
      "Epoch 12/100\n",
      "568/568 [==============================] - 27s 48ms/step - loss: -5.4140e-04 - KLD: -5.4140e-04 - val_loss: -5.3933e-04 - val_KLD: -5.3933e-04\n",
      "Epoch 13/100\n",
      "568/568 [==============================] - 26s 46ms/step - loss: -5.4193e-04 - KLD: -5.4193e-04 - val_loss: -5.3988e-04 - val_KLD: -5.3988e-04\n",
      "Epoch 14/100\n",
      "568/568 [==============================] - 25s 44ms/step - loss: -5.4248e-04 - KLD: -5.4248e-04 - val_loss: -5.4023e-04 - val_KLD: -5.4023e-04\n",
      "Epoch 15/100\n",
      "568/568 [==============================] - 25s 45ms/step - loss: -5.4284e-04 - KLD: -5.4284e-04 - val_loss: -5.4053e-04 - val_KLD: -5.4053e-04\n",
      "Epoch 16/100\n",
      "568/568 [==============================] - 26s 46ms/step - loss: -5.4327e-04 - KLD: -5.4327e-04 - val_loss: -5.4082e-04 - val_KLD: -5.4082e-04\n",
      "Epoch 17/100\n",
      "568/568 [==============================] - 26s 45ms/step - loss: -5.4361e-04 - KLD: -5.4361e-04 - val_loss: -5.4102e-04 - val_KLD: -5.4102e-04\n",
      "Epoch 18/100\n",
      "568/568 [==============================] - 28s 49ms/step - loss: -5.4388e-04 - KLD: -5.4388e-04 - val_loss: -5.4136e-04 - val_KLD: -5.4136e-04\n",
      "Epoch 19/100\n",
      "568/568 [==============================] - 26s 46ms/step - loss: -5.4417e-04 - KLD: -5.4417e-04 - val_loss: -5.4144e-04 - val_KLD: -5.4144e-04\n",
      "Epoch 20/100\n",
      "568/568 [==============================] - 30s 52ms/step - loss: -5.4446e-04 - KLD: -5.4446e-04 - val_loss: -5.4166e-04 - val_KLD: -5.4166e-04\n",
      "Epoch 21/100\n",
      "568/568 [==============================] - 28s 49ms/step - loss: -5.4468e-04 - KLD: -5.4468e-04 - val_loss: -5.4189e-04 - val_KLD: -5.4189e-04\n",
      "Epoch 22/100\n",
      "568/568 [==============================] - 25s 44ms/step - loss: -5.4491e-04 - KLD: -5.4491e-04 - val_loss: -5.4197e-04 - val_KLD: -5.4197e-04\n",
      "Epoch 23/100\n",
      "568/568 [==============================] - 27s 47ms/step - loss: -5.4510e-04 - KLD: -5.4510e-04 - val_loss: -5.4212e-04 - val_KLD: -5.4212e-04\n",
      "Epoch 24/100\n",
      "568/568 [==============================] - 26s 45ms/step - loss: -5.4531e-04 - KLD: -5.4531e-04 - val_loss: -5.4228e-04 - val_KLD: -5.4228e-04\n",
      "Epoch 25/100\n",
      "568/568 [==============================] - 28s 49ms/step - loss: -5.4549e-04 - KLD: -5.4549e-04 - val_loss: -5.4243e-04 - val_KLD: -5.4243e-04\n",
      "Epoch 26/100\n",
      "568/568 [==============================] - 26s 47ms/step - loss: -5.4566e-04 - KLD: -5.4566e-04 - val_loss: -5.4254e-04 - val_KLD: -5.4254e-04\n",
      "Epoch 27/100\n",
      "568/568 [==============================] - 26s 46ms/step - loss: -5.4578e-04 - KLD: -5.4578e-04 - val_loss: -5.4258e-04 - val_KLD: -5.4258e-04\n",
      "Epoch 28/100\n",
      "568/568 [==============================] - 28s 49ms/step - loss: -5.4597e-04 - KLD: -5.4597e-04 - val_loss: -5.4274e-04 - val_KLD: -5.4274e-04\n",
      "Epoch 29/100\n",
      "568/568 [==============================] - 27s 47ms/step - loss: -5.4610e-04 - KLD: -5.4610e-04 - val_loss: -5.4280e-04 - val_KLD: -5.4280e-04\n",
      "Epoch 30/100\n",
      "568/568 [==============================] - 29s 50ms/step - loss: -5.4623e-04 - KLD: -5.4623e-04 - val_loss: -5.4290e-04 - val_KLD: -5.4290e-04\n",
      "Epoch 31/100\n",
      "568/568 [==============================] - 27s 47ms/step - loss: -5.4637e-04 - KLD: -5.4637e-04 - val_loss: -5.4303e-04 - val_KLD: -5.4303e-04\n",
      "Epoch 32/100\n",
      "568/568 [==============================] - 27s 48ms/step - loss: -5.4644e-04 - KLD: -5.4644e-04 - val_loss: -5.4310e-04 - val_KLD: -5.4310e-04\n",
      "Epoch 33/100\n",
      "568/568 [==============================] - 28s 49ms/step - loss: -5.4657e-04 - KLD: -5.4657e-04 - val_loss: -5.4315e-04 - val_KLD: -5.4315e-04\n",
      "Epoch 34/100\n",
      "568/568 [==============================] - 27s 47ms/step - loss: -5.4672e-04 - KLD: -5.4672e-04 - val_loss: -5.4321e-04 - val_KLD: -5.4321e-04\n",
      "Epoch 35/100\n",
      "568/568 [==============================] - 27s 47ms/step - loss: -5.4678e-04 - KLD: -5.4678e-04 - val_loss: -5.4324e-04 - val_KLD: -5.4324e-04\n",
      "Epoch 36/100\n",
      "568/568 [==============================] - 27s 47ms/step - loss: -5.4690e-04 - KLD: -5.4690e-04 - val_loss: -5.4340e-04 - val_KLD: -5.4340e-04\n",
      "Epoch 37/100\n",
      "568/568 [==============================] - 27s 47ms/step - loss: -5.4700e-04 - KLD: -5.4700e-04 - val_loss: -5.4339e-04 - val_KLD: -5.4339e-04\n",
      "Epoch 38/100\n",
      "568/568 [==============================] - 27s 47ms/step - loss: -5.4707e-04 - KLD: -5.4707e-04 - val_loss: -5.4348e-04 - val_KLD: -5.4348e-04\n",
      "Epoch 39/100\n",
      "568/568 [==============================] - 26s 45ms/step - loss: -5.4716e-04 - KLD: -5.4716e-04 - val_loss: -5.4346e-04 - val_KLD: -5.4346e-04\n",
      "Epoch 40/100\n",
      "568/568 [==============================] - 27s 48ms/step - loss: -5.4722e-04 - KLD: -5.4722e-04 - val_loss: -5.4355e-04 - val_KLD: -5.4355e-04\n",
      "Epoch 41/100\n",
      "568/568 [==============================] - 26s 46ms/step - loss: -5.4730e-04 - KLD: -5.4730e-04 - val_loss: -5.4357e-04 - val_KLD: -5.4357e-04\n",
      "Epoch 42/100\n",
      "568/568 [==============================] - 27s 48ms/step - loss: -5.4737e-04 - KLD: -5.4737e-04 - val_loss: -5.4360e-04 - val_KLD: -5.4360e-04\n",
      "Epoch 43/100\n",
      "568/568 [==============================] - 26s 46ms/step - loss: -5.4745e-04 - KLD: -5.4745e-04 - val_loss: -5.4369e-04 - val_KLD: -5.4369e-04\n",
      "Epoch 44/100\n",
      "568/568 [==============================] - 26s 45ms/step - loss: -5.4754e-04 - KLD: -5.4754e-04 - val_loss: -5.4372e-04 - val_KLD: -5.4372e-04\n",
      "Epoch 45/100\n",
      "568/568 [==============================] - 27s 47ms/step - loss: -5.4762e-04 - KLD: -5.4762e-04 - val_loss: -5.4376e-04 - val_KLD: -5.4376e-04\n",
      "Epoch 46/100\n",
      "568/568 [==============================] - 26s 45ms/step - loss: -5.4768e-04 - KLD: -5.4768e-04 - val_loss: -5.4382e-04 - val_KLD: -5.4382e-04\n",
      "Epoch 47/100\n",
      "568/568 [==============================] - 28s 50ms/step - loss: -5.4773e-04 - KLD: -5.4773e-04 - val_loss: -5.4395e-04 - val_KLD: -5.4395e-04\n",
      "Epoch 48/100\n",
      "568/568 [==============================] - 27s 48ms/step - loss: -5.4781e-04 - KLD: -5.4781e-04 - val_loss: -5.4390e-04 - val_KLD: -5.4390e-04\n",
      "Epoch 49/100\n",
      "568/568 [==============================] - 26s 46ms/step - loss: -5.4786e-04 - KLD: -5.4786e-04 - val_loss: -5.4386e-04 - val_KLD: -5.4386e-04\n",
      "Epoch 50/100\n",
      "568/568 [==============================] - 27s 48ms/step - loss: -5.4789e-04 - KLD: -5.4789e-04 - val_loss: -5.4392e-04 - val_KLD: -5.4392e-04\n",
      "Epoch 51/100\n",
      "568/568 [==============================] - 26s 46ms/step - loss: -5.4797e-04 - KLD: -5.4797e-04 - val_loss: -5.4399e-04 - val_KLD: -5.4399e-04\n",
      "Epoch 52/100\n",
      "568/568 [==============================] - 28s 50ms/step - loss: -5.4803e-04 - KLD: -5.4803e-04 - val_loss: -5.4402e-04 - val_KLD: -5.4402e-04\n",
      "Epoch 53/100\n",
      "568/568 [==============================] - 26s 46ms/step - loss: -5.4806e-04 - KLD: -5.4806e-04 - val_loss: -5.4407e-04 - val_KLD: -5.4407e-04\n",
      "Epoch 54/100\n",
      "568/568 [==============================] - 27s 47ms/step - loss: -5.4813e-04 - KLD: -5.4813e-04 - val_loss: -5.4404e-04 - val_KLD: -5.4404e-04\n",
      "Epoch 55/100\n",
      "568/568 [==============================] - 27s 47ms/step - loss: -5.4817e-04 - KLD: -5.4817e-04 - val_loss: -5.4409e-04 - val_KLD: -5.4409e-04\n",
      "Epoch 56/100\n",
      "568/568 [==============================] - 26s 45ms/step - loss: -5.4821e-04 - KLD: -5.4821e-04 - val_loss: -5.4415e-04 - val_KLD: -5.4415e-04\n",
      "Epoch 57/100\n",
      "568/568 [==============================] - 28s 49ms/step - loss: -5.4826e-04 - KLD: -5.4826e-04 - val_loss: -5.4418e-04 - val_KLD: -5.4418e-04\n",
      "Epoch 58/100\n",
      "568/568 [==============================] - 26s 46ms/step - loss: -5.4830e-04 - KLD: -5.4830e-04 - val_loss: -5.4419e-04 - val_KLD: -5.4419e-04\n",
      "Epoch 59/100\n",
      "568/568 [==============================] - 26s 46ms/step - loss: -5.4837e-04 - KLD: -5.4837e-04 - val_loss: -5.4422e-04 - val_KLD: -5.4422e-04\n",
      "Epoch 60/100\n",
      "568/568 [==============================] - 28s 49ms/step - loss: -5.4841e-04 - KLD: -5.4841e-04 - val_loss: -5.4413e-04 - val_KLD: -5.4413e-04\n",
      "Epoch 61/100\n",
      "568/568 [==============================] - 26s 46ms/step - loss: -5.4844e-04 - KLD: -5.4844e-04 - val_loss: -5.4433e-04 - val_KLD: -5.4433e-04\n",
      "Epoch 62/100\n",
      "568/568 [==============================] - 28s 49ms/step - loss: -5.4849e-04 - KLD: -5.4849e-04 - val_loss: -5.4431e-04 - val_KLD: -5.4431e-04\n",
      "Epoch 63/100\n",
      "568/568 [==============================] - 26s 46ms/step - loss: -5.4852e-04 - KLD: -5.4852e-04 - val_loss: -5.4434e-04 - val_KLD: -5.4434e-04\n",
      "Epoch 64/100\n",
      "568/568 [==============================] - 26s 45ms/step - loss: -5.4859e-04 - KLD: -5.4859e-04 - val_loss: -5.4430e-04 - val_KLD: -5.4430e-04\n",
      "Epoch 65/100\n",
      "568/568 [==============================] - 28s 49ms/step - loss: -5.4860e-04 - KLD: -5.4860e-04 - val_loss: -5.4437e-04 - val_KLD: -5.4437e-04\n",
      "Epoch 66/100\n",
      "568/568 [==============================] - 27s 48ms/step - loss: -5.4864e-04 - KLD: -5.4864e-04 - val_loss: -5.4436e-04 - val_KLD: -5.4436e-04\n",
      "Epoch 67/100\n",
      "568/568 [==============================] - 29s 51ms/step - loss: -5.4866e-04 - KLD: -5.4866e-04 - val_loss: -5.4444e-04 - val_KLD: -5.4444e-04\n",
      "Epoch 68/100\n",
      "568/568 [==============================] - 26s 45ms/step - loss: -5.4869e-04 - KLD: -5.4869e-04 - val_loss: -5.4443e-04 - val_KLD: -5.4443e-04\n",
      "Epoch 69/100\n",
      "568/568 [==============================] - 27s 48ms/step - loss: -5.4873e-04 - KLD: -5.4873e-04 - val_loss: -5.4442e-04 - val_KLD: -5.4442e-04\n",
      "Epoch 70/100\n",
      "568/568 [==============================] - 27s 48ms/step - loss: -5.4877e-04 - KLD: -5.4877e-04 - val_loss: -5.4451e-04 - val_KLD: -5.4451e-04\n",
      "Epoch 71/100\n",
      "568/568 [==============================] - 26s 46ms/step - loss: -5.4878e-04 - KLD: -5.4878e-04 - val_loss: -5.4450e-04 - val_KLD: -5.4450e-04\n",
      "Epoch 72/100\n",
      "568/568 [==============================] - 27s 48ms/step - loss: -5.4884e-04 - KLD: -5.4884e-04 - val_loss: -5.4445e-04 - val_KLD: -5.4445e-04\n",
      "Epoch 73/100\n",
      "568/568 [==============================] - 26s 45ms/step - loss: -5.4888e-04 - KLD: -5.4888e-04 - val_loss: -5.4453e-04 - val_KLD: -5.4453e-04\n",
      "Epoch 74/100\n",
      "568/568 [==============================] - 27s 48ms/step - loss: -5.4888e-04 - KLD: -5.4888e-04 - val_loss: -5.4451e-04 - val_KLD: -5.4451e-04\n",
      "Epoch 75/100\n",
      "568/568 [==============================] - 26s 46ms/step - loss: -5.4893e-04 - KLD: -5.4893e-04 - val_loss: -5.4457e-04 - val_KLD: -5.4457e-04\n",
      "Epoch 76/100\n",
      "568/568 [==============================] - 27s 47ms/step - loss: -5.4894e-04 - KLD: -5.4894e-04 - val_loss: -5.4461e-04 - val_KLD: -5.4461e-04\n",
      "Epoch 77/100\n",
      "568/568 [==============================] - 28s 49ms/step - loss: -5.4898e-04 - KLD: -5.4898e-04 - val_loss: -5.4466e-04 - val_KLD: -5.4466e-04\n",
      "Epoch 78/100\n",
      "568/568 [==============================] - 27s 48ms/step - loss: -5.4902e-04 - KLD: -5.4902e-04 - val_loss: -5.4463e-04 - val_KLD: -5.4463e-04\n",
      "Epoch 79/100\n",
      "568/568 [==============================] - 28s 49ms/step - loss: -5.4905e-04 - KLD: -5.4905e-04 - val_loss: -5.4457e-04 - val_KLD: -5.4457e-04\n",
      "Epoch 80/100\n",
      "568/568 [==============================] - 26s 45ms/step - loss: -5.4905e-04 - KLD: -5.4905e-04 - val_loss: -5.4462e-04 - val_KLD: -5.4462e-04\n",
      "Epoch 81/100\n",
      "568/568 [==============================] - 26s 46ms/step - loss: -5.4910e-04 - KLD: -5.4910e-04 - val_loss: -5.4462e-04 - val_KLD: -5.4462e-04\n",
      "Epoch 82/100\n",
      "568/568 [==============================] - 27s 47ms/step - loss: -5.4912e-04 - KLD: -5.4912e-04 - val_loss: -5.4468e-04 - val_KLD: -5.4468e-04\n",
      "Epoch 83/100\n",
      "568/568 [==============================] - 25s 44ms/step - loss: -5.4915e-04 - KLD: -5.4915e-04 - val_loss: -5.4463e-04 - val_KLD: -5.4463e-04\n",
      "Epoch 84/100\n",
      "568/568 [==============================] - 26s 45ms/step - loss: -5.4918e-04 - KLD: -5.4918e-04 - val_loss: -5.4468e-04 - val_KLD: -5.4468e-04\n",
      "Epoch 85/100\n",
      "568/568 [==============================] - 25s 44ms/step - loss: -5.4919e-04 - KLD: -5.4919e-04 - val_loss: -5.4471e-04 - val_KLD: -5.4471e-04\n",
      "Epoch 86/100\n",
      "568/568 [==============================] - 25s 45ms/step - loss: -5.4922e-04 - KLD: -5.4922e-04 - val_loss: -5.4469e-04 - val_KLD: -5.4469e-04\n",
      "Epoch 87/100\n",
      "568/568 [==============================] - 27s 47ms/step - loss: -5.4924e-04 - KLD: -5.4924e-04 - val_loss: -5.4469e-04 - val_KLD: -5.4469e-04\n",
      "Epoch 88/100\n",
      "568/568 [==============================] - 25s 45ms/step - loss: -5.4928e-04 - KLD: -5.4928e-04 - val_loss: -5.4471e-04 - val_KLD: -5.4471e-04\n",
      "Epoch 89/100\n",
      "568/568 [==============================] - 27s 47ms/step - loss: -5.4929e-04 - KLD: -5.4929e-04 - val_loss: -5.4478e-04 - val_KLD: -5.4478e-04\n",
      "Epoch 90/100\n",
      "568/568 [==============================] - 26s 46ms/step - loss: -5.4931e-04 - KLD: -5.4931e-04 - val_loss: -5.4473e-04 - val_KLD: -5.4473e-04\n",
      "Epoch 91/100\n",
      "568/568 [==============================] - 27s 47ms/step - loss: -5.4934e-04 - KLD: -5.4934e-04 - val_loss: -5.4475e-04 - val_KLD: -5.4475e-04\n",
      "Epoch 92/100\n",
      "568/568 [==============================] - 28s 50ms/step - loss: -5.4935e-04 - KLD: -5.4935e-04 - val_loss: -5.4474e-04 - val_KLD: -5.4474e-04\n",
      "Epoch 93/100\n",
      "568/568 [==============================] - 27s 47ms/step - loss: -5.4937e-04 - KLD: -5.4937e-04 - val_loss: -5.4479e-04 - val_KLD: -5.4479e-04\n",
      "Epoch 94/100\n",
      "568/568 [==============================] - 28s 49ms/step - loss: -5.4941e-04 - KLD: -5.4941e-04 - val_loss: -5.4479e-04 - val_KLD: -5.4479e-04\n",
      "Epoch 95/100\n",
      "568/568 [==============================] - 27s 47ms/step - loss: -5.4943e-04 - KLD: -5.4943e-04 - val_loss: -5.4480e-04 - val_KLD: -5.4480e-04\n",
      "Epoch 96/100\n",
      "568/568 [==============================] - 26s 47ms/step - loss: -5.4944e-04 - KLD: -5.4944e-04 - val_loss: -5.4481e-04 - val_KLD: -5.4481e-04\n",
      "Epoch 97/100\n",
      "568/568 [==============================] - 27s 48ms/step - loss: -5.4946e-04 - KLD: -5.4946e-04 - val_loss: -5.4487e-04 - val_KLD: -5.4487e-04\n",
      "Epoch 98/100\n",
      "568/568 [==============================] - 27s 47ms/step - loss: -5.4949e-04 - KLD: -5.4949e-04 - val_loss: -5.4480e-04 - val_KLD: -5.4480e-04\n",
      "Epoch 99/100\n",
      "568/568 [==============================] - 28s 50ms/step - loss: -5.4949e-04 - KLD: -5.4949e-04 - val_loss: -5.4488e-04 - val_KLD: -5.4488e-04\n",
      "Epoch 100/100\n",
      "568/568 [==============================] - 26s 46ms/step - loss: -5.4952e-04 - KLD: -5.4952e-04 - val_loss: -5.4481e-04 - val_KLD: -5.4481e-04\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Layer \"model0\" expects 2 input(s), but it received 1 input tensors. Inputs received: [<tf.Tensor: shape=(32, 800), dtype=float32, numpy=\narray([[-5.88153696e+00, -2.38435864e+00,  5.66012335e+00, ...,\n         9.61677507e-02, -9.74584520e-02,  8.66410434e-02],\n       [-3.09530020e+00,  1.29145203e+01, -1.27189665e+01, ...,\n        -1.75717920e-01, -1.99270368e-01,  5.82260899e-02],\n       [-1.34647989e+01, -6.06159639e+00,  8.60588551e+00, ...,\n         8.07968061e-03, -4.89619970e-02, -2.03010812e-01],\n       ...,\n       [ 1.85393829e+01, -1.00781107e+01,  1.42456427e+01, ...,\n         6.72174469e-02, -3.39383748e-03, -5.51992580e-02],\n       [-2.16536369e+01, -2.39834309e+00,  4.86847305e+00, ...,\n         2.07342952e-01, -6.52381685e-03,  4.72200364e-02],\n       [-1.86684494e+01,  2.44482493e+00, -3.40622544e+00, ...,\n        -2.03052163e-01,  3.98116320e-01, -1.07740834e-01]], dtype=float32)>]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 67\u001b[0m\n\u001b[0;32m     55\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(\n\u001b[0;32m     56\u001b[0m     [X_train, X_trainLGBM],\n\u001b[0;32m     57\u001b[0m     y_train,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     62\u001b[0m     verbose\u001b[38;5;241m=\u001b[39mverbose\n\u001b[0;32m     63\u001b[0m )\n\u001b[0;32m     65\u001b[0m model\u001b[38;5;241m.\u001b[39msave(directory \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_combined.keras\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 67\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     69\u001b[0m kld \u001b[38;5;241m=\u001b[39m KLDivergence()\n\u001b[0;32m     70\u001b[0m \u001b[38;5;28mprint\u001b[39m(kld(y_test, predictions), JensenShannonDiv(y_test, predictions), wJensenShannonDiv(y_test, predictions))\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\input_spec.py:216\u001b[0m, in \u001b[0;36massert_input_compatibility\u001b[1;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[0;32m    213\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInputs to a layer should be tensors. Got: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(inputs) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(input_spec):\n\u001b[1;32m--> 216\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    217\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLayer \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m expects \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(input_spec)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m input(s),\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    218\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m but it received \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(inputs)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m input tensors. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    219\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInputs received: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minputs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    220\u001b[0m     )\n\u001b[0;32m    221\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m input_index, (x, spec) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mzip\u001b[39m(inputs, input_spec)):\n\u001b[0;32m    222\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m spec \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mValueError\u001b[0m: Layer \"model0\" expects 2 input(s), but it received 1 input tensors. Inputs received: [<tf.Tensor: shape=(32, 800), dtype=float32, numpy=\narray([[-5.88153696e+00, -2.38435864e+00,  5.66012335e+00, ...,\n         9.61677507e-02, -9.74584520e-02,  8.66410434e-02],\n       [-3.09530020e+00,  1.29145203e+01, -1.27189665e+01, ...,\n        -1.75717920e-01, -1.99270368e-01,  5.82260899e-02],\n       [-1.34647989e+01, -6.06159639e+00,  8.60588551e+00, ...,\n         8.07968061e-03, -4.89619970e-02, -2.03010812e-01],\n       ...,\n       [ 1.85393829e+01, -1.00781107e+01,  1.42456427e+01, ...,\n         6.72174469e-02, -3.39383748e-03, -5.51992580e-02],\n       [-2.16536369e+01, -2.39834309e+00,  4.86847305e+00, ...,\n         2.07342952e-01, -6.52381685e-03,  4.72200364e-02],\n       [-1.86684494e+01,  2.44482493e+00, -3.40622544e+00, ...,\n        -2.03052163e-01,  3.98116320e-01, -1.07740834e-01]], dtype=float32)>]"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "PCAmodels = {}\n",
    "\n",
    "batch_size = 128\n",
    "epochs = 100\n",
    "verbose = 1\n",
    "\n",
    "n_splits = 5\n",
    "n = 0\n",
    "\n",
    "# K-fold Cross Validation model evaluation\n",
    "\n",
    "X_train, X_test, y_train, y_test, X_trainLGBM, X_testLGBM  = train_test_split(X_data, Y_data_df, CV_predictions, test_size = 0.2)                                                                       \n",
    "\n",
    "PCAmodels[n], X_train = dim_reduct(X_train, 800)\n",
    "X_test = apply_PCA(X_test, PCAmodels[n])\n",
    "# X_train = np.expand_dims(X_train, axis=2)\n",
    "# X_test = np.expand_dims(X_test, axis=2)\n",
    "\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_test shape:\", y_test.shape)\n",
    "print(\"X_trainLGBM shape\", X_trainLGBM.shape)\n",
    "print(\"X_testLGBM shape\", X_testLGBM.shape)\n",
    "\n",
    "# mA0 = keras.layers.Input(shape=(X_train.shape[1], X_train.shape[2]), name = \"inputA\")\n",
    "mA0 = keras.layers.Input(shape=X_train.shape[1], name = \"inputA\")\n",
    "mA1 = keras.layers.Dense(1200, activation=\"gelu\")(mA0)\n",
    "mA2 = keras.layers.Dropout(0.05)(mA1)\n",
    "mA3 = keras.layers.Dense(600, activation=\"gelu\")(mA2)\n",
    "mA4 = keras.layers.Dropout(0.05)(mA3)\n",
    "mA5 = keras.layers.Dense(200, activation=\"gelu\")(mA4)\n",
    "mA6 = keras.layers.Dropout(0.05)(mA5)\n",
    "mA7 = keras.layers.Dense(24, activation=\"gelu\")(mA6)\n",
    "\n",
    "mB0 = keras.layers.Input(shape=X_trainLGBM.shape[1], name = \"inputB\")\n",
    "\n",
    "mAB = keras.layers.concatenate([mA7,mB0],name=\"concatenated_layer\")\n",
    "output_layer = keras.layers.Dense(6, activation = \"softmax\")(mAB)\n",
    "\n",
    "model = keras.models.Model(inputs=[mA0,mB0],outputs=[output_layer], name = f'model{n}')\n",
    "\n",
    "model.compile(\n",
    "    loss= keras.losses.KLDivergence(),\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=1e-4),\n",
    "    metrics=[keras.metrics.KLDivergence(name = \"KLD\")],\n",
    "    run_eagerly=True\n",
    ")\n",
    "if n == 0: print(model.summary())\n",
    "\n",
    "callbacks = [keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=5)]\n",
    "\n",
    "model.fit(\n",
    "    [X_train, X_trainLGBM],\n",
    "    y_train,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_split=0.15,\n",
    "    callbacks=callbacks,\n",
    "    verbose=verbose\n",
    ")\n",
    "\n",
    "model.save(directory + f\"model_combined.keras\")\n",
    "\n",
    "predictions = model.predict([X_test, X_testLGBM])\n",
    "\n",
    "kld = KLDivergence()\n",
    "print(kld(y_test, predictions), JensenShannonDiv(y_test, predictions), wJensenShannonDiv(y_test, predictions))\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5db1c009-7fd6-4f7b-9866-3515f75addb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "668/668 [==============================] - 5s 7ms/step\n",
      "tf.Tensor(-0.00054504786, shape=(), dtype=float32) tf.Tensor(0.009731499, shape=(), dtype=float32) tf.Tensor(1.4608102e-05, shape=(), dtype=float32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2578"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = model.predict([X_test, X_testLGBM])\n",
    "\n",
    "kld = KLDivergence()\n",
    "print(kld(y_test, predictions), JensenShannonDiv(y_test, predictions), wJensenShannonDiv(y_test, predictions))\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee54868-9498-48b6-b33e-b3099102c6d2",
   "metadata": {},
   "source": [
    "perf combined model: The metrics unanimously described this model as better than the non-combined one\n",
    "KLDivergence -0.00054504786\n",
    "JensenShannonDiv: 009731499\n",
    "wJensenShannonDiv : 1.4608102e-05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "54d5e068-dc0f-4bef-9aa5-6ce289bba317",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save best model\n",
    "class cPCAmodels:\n",
    "    def __init__(self):\n",
    "        self.model = 0\n",
    "\n",
    "PCAm = cPCAmodels()\n",
    "PCAm.model = PCAmodels[0]\n",
    "file = open(directory + \"PCAmodel_attempt5.pickle\", 'wb')\n",
    "pickle.dump(PCAm, file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "08f755b1-bc89-4376-854e-e2e7f0710441",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras.engine.functional.Functional object at 0x00000242E37F8550>\n"
     ]
    }
   ],
   "source": [
    "# the best model is model 1\n",
    "model = keras.models.load_model(directory + \"model_combined.keras\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3dd5cf86-43c6-41d1-8624-0655e01bdf77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "835/835 [==============================] - 7s 8ms/step - loss: -5.4780e-04 - KLD: -5.4780e-04\n",
      "Epoch 2/10\n",
      "835/835 [==============================] - 6s 8ms/step - loss: -5.4815e-04 - KLD: -5.4815e-04\n",
      "Epoch 3/10\n",
      "835/835 [==============================] - 6s 8ms/step - loss: -5.4830e-04 - KLD: -5.4830e-04\n",
      "Epoch 4/10\n",
      "835/835 [==============================] - 6s 8ms/step - loss: -5.4842e-04 - KLD: -5.4842e-04\n",
      "Epoch 5/10\n",
      "835/835 [==============================] - 7s 8ms/step - loss: -5.4851e-04 - KLD: -5.4851e-04\n",
      "Epoch 6/10\n",
      "835/835 [==============================] - 7s 8ms/step - loss: -5.4857e-04 - KLD: -5.4857e-04\n",
      "Epoch 7/10\n",
      "835/835 [==============================] - 7s 8ms/step - loss: -5.4864e-04 - KLD: -5.4864e-04\n",
      "Epoch 8/10\n",
      "835/835 [==============================] - 6s 8ms/step - loss: -5.4872e-04 - KLD: -5.4872e-04\n",
      "Epoch 9/10\n",
      "835/835 [==============================] - 7s 8ms/step - loss: -5.4877e-04 - KLD: -5.4877e-04\n",
      "Epoch 10/10\n",
      "835/835 [==============================] - 7s 8ms/step - loss: -5.4881e-04 - KLD: -5.4881e-04\n"
     ]
    }
   ],
   "source": [
    "# five more epoch with the entire dataset\n",
    "\n",
    "n_attempt = 5\n",
    "batch_size = 128\n",
    "epochs = 10\n",
    "verbose = 1\n",
    "\n",
    "X_train = apply_PCA(X_data, PCAmodels[0])\n",
    "\n",
    "model.fit(\n",
    "    [X_train, CV_predictions],\n",
    "    Y_data_df,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    verbose=verbose\n",
    ")\n",
    "\n",
    "model.save(directory + f\"finalmodel{n_attempt}.keras\", save_format='keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc1a1de-c87e-454f-814b-8cd7fc81fd8e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
